Found 15 statistics files:
  - results_fibonacci_go_a1/fibonacci_go_a1_statistics.csv
  - results_fibonacci_go_a2/fibonacci_go_a2_statistics.csv
  - results_fibonacci_go_a3/fibonacci_go_a3_statistics.csv
  - results_fibonacci_go_a4/fibonacci_go_a4_statistics.csv
  - results_fibonacci_go_a5/fibonacci_go_a5_statistics.csv
  - results_fibonacci_nodejs_a1/fibonacci_nodejs_a1_statistics.csv
  - results_fibonacci_nodejs_a2/fibonacci_nodejs_a2_statistics.csv
  - results_fibonacci_nodejs_a3/fibonacci_nodejs_a3_statistics.csv
  - results_fibonacci_nodejs_a4/fibonacci_nodejs_a4_statistics.csv
  - results_fibonacci_nodejs_a5/fibonacci_nodejs_a5_statistics.csv
  - results_fibonacci_python_a1/fibonacci_python_a1_statistics.csv
  - results_fibonacci_python_a2/fibonacci_python_a2_statistics.csv
  - results_fibonacci_python_a3/fibonacci_python_a3_statistics.csv
  - results_fibonacci_python_a4/fibonacci_python_a4_statistics.csv
  - results_fibonacci_python_a5/fibonacci_python_a5_statistics.csv

================================================================================
MULTI-POD COST ANALYSIS: A/B/C SERIES COMPARISON
Multiple RPS levels: 100, 200, 300, 400, 500 RPS
================================================================================

--- Processing: fibonacci_go_a1_statistics.csv ---
Found achieved RPS data: results_fibonacci_go_a1/achieved_rps_summary.csv
Loaded achieved RPS for targets: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]
Detected: Runtime=go, Config=A1, Series=A
Configuration: 1.0vCPU × 1.0GB × 1 pods (concurrency: 1/pod)
Found data for RPS levels: [100, 200, 300, 400, 500]
  Using achieved RPS from summary: 100.0 -> 98.30 (98.3% efficiency)
  100.0 RPS: 1/1 pods busy (avg latency: 28677μs, N_in_service: 2.8, utilization: 281.9%)
  Using achieved RPS from summary: 200.0 -> 122.38 (61.2% efficiency)
  200.0 RPS: 1/1 pods busy (avg latency: 9587097μs, N_in_service: 1173.3, utilization: 117326.9%)
  Using achieved RPS from summary: 300.0 -> 117.59 (39.2% efficiency)
  300.0 RPS: 1/1 pods busy (avg latency: 17700732μs, N_in_service: 2081.4, utilization: 208142.9%)
  Using achieved RPS from summary: 400.0 -> 97.97 (24.5% efficiency)
  400.0 RPS: 1/1 pods busy (avg latency: 20796556μs, N_in_service: 2037.4, utilization: 203743.9%)
  Using achieved RPS from summary: 500.0 -> 101.13 (20.2% efficiency)
  500.0 RPS: 1/1 pods busy (avg latency: 10365653μs, N_in_service: 1048.3, utilization: 104827.9%)
Processed: 5 configurations

--- Processing: fibonacci_go_a2_statistics.csv ---
Found achieved RPS data: results_fibonacci_go_a2/achieved_rps_summary.csv
Loaded achieved RPS for targets: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]
Detected: Runtime=go, Config=A2, Series=A
Configuration: 0.5vCPU × 0.5GB × 1 pods (concurrency: 1/pod)
Found data for RPS levels: [100, 200, 300, 400, 500]
  Using achieved RPS from summary: 100.0 -> 99.11 (99.1% efficiency)
  100.0 RPS: 1/1 pods busy (avg latency: 25611μs, N_in_service: 2.5, utilization: 253.8%)
  Using achieved RPS from summary: 200.0 -> 134.27 (67.1% efficiency)
  200.0 RPS: 1/1 pods busy (avg latency: 8741964μs, N_in_service: 1173.8, utilization: 117378.4%)
  Using achieved RPS from summary: 300.0 -> 117.14 (39.0% efficiency)
  300.0 RPS: 1/1 pods busy (avg latency: 18726655μs, N_in_service: 2193.6, utilization: 219364.0%)
  Using achieved RPS from summary: 400.0 -> 103.61 (25.9% efficiency)
  400.0 RPS: 1/1 pods busy (avg latency: 20025157μs, N_in_service: 2074.8, utilization: 207480.6%)
  Using achieved RPS from summary: 500.0 -> 106.81 (21.4% efficiency)
  500.0 RPS: 1/1 pods busy (avg latency: 19284858μs, N_in_service: 2059.8, utilization: 205981.6%)
Processed: 5 configurations

--- Processing: fibonacci_go_a3_statistics.csv ---
Found achieved RPS data: results_fibonacci_go_a3/achieved_rps_summary.csv
Loaded achieved RPS for targets: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]
Detected: Runtime=go, Config=A3, Series=A
Configuration: 0.25vCPU × 0.25GB × 1 pods (concurrency: 1/pod)
Found data for RPS levels: [100, 200, 300, 400, 500]
  Using achieved RPS from summary: 100.0 -> 99.15 (99.1% efficiency)
  100.0 RPS: 1/1 pods busy (avg latency: 27633μs, N_in_service: 2.7, utilization: 274.0%)
  Using achieved RPS from summary: 200.0 -> 132.64 (66.3% efficiency)
  200.0 RPS: 1/1 pods busy (avg latency: 9222527μs, N_in_service: 1223.3, utilization: 122327.6%)
  Using achieved RPS from summary: 300.0 -> 116.40 (38.8% efficiency)
  300.0 RPS: 1/1 pods busy (avg latency: 18876371μs, N_in_service: 2197.2, utilization: 219721.0%)
  Using achieved RPS from summary: 400.0 -> 99.59 (24.9% efficiency)
  400.0 RPS: 1/1 pods busy (avg latency: 20944024μs, N_in_service: 2085.8, utilization: 208581.5%)
  Using achieved RPS from summary: 500.0 -> 102.88 (20.6% efficiency)
  500.0 RPS: 1/1 pods busy (avg latency: 9229367μs, N_in_service: 949.5, utilization: 94951.7%)
Processed: 5 configurations

--- Processing: fibonacci_go_a4_statistics.csv ---
Found achieved RPS data: results_fibonacci_go_a4/achieved_rps_summary.csv
Loaded achieved RPS for targets: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]
Detected: Runtime=go, Config=A4, Series=A
Configuration: 0.125vCPU × 0.125GB × 1 pods (concurrency: 1/pod)
Found data for RPS levels: [100, 200, 300, 400, 500]
  Using achieved RPS from summary: 100.0 -> 99.32 (99.3% efficiency)
  100.0 RPS: 1/1 pods busy (avg latency: 25996μs, N_in_service: 2.6, utilization: 258.2%)
  Using achieved RPS from summary: 200.0 -> 132.54 (66.3% efficiency)
  200.0 RPS: 1/1 pods busy (avg latency: 9079727μs, N_in_service: 1203.4, utilization: 120342.7%)
  Using achieved RPS from summary: 300.0 -> 113.96 (38.0% efficiency)
  300.0 RPS: 1/1 pods busy (avg latency: 18871690μs, N_in_service: 2150.6, utilization: 215061.8%)
  Using achieved RPS from summary: 400.0 -> 100.62 (25.2% efficiency)
  400.0 RPS: 1/1 pods busy (avg latency: 18989443μs, N_in_service: 1910.7, utilization: 191071.8%)
  Using achieved RPS from summary: 500.0 -> 105.37 (21.1% efficiency)
  500.0 RPS: 1/1 pods busy (avg latency: 15245581μs, N_in_service: 1606.4, utilization: 160642.7%)
Processed: 5 configurations

--- Processing: fibonacci_go_a5_statistics.csv ---
Found achieved RPS data: results_fibonacci_go_a5/achieved_rps_summary.csv
Loaded achieved RPS for targets: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]
Detected: Runtime=go, Config=A5, Series=A
Configuration: 0.1vCPU × 0.1GB × 1 pods (concurrency: 1/pod)
Found data for RPS levels: [100, 200, 300, 400, 500]
  Using achieved RPS from summary: 100.0 -> 99.42 (99.4% efficiency)
  100.0 RPS: 1/1 pods busy (avg latency: 29402μs, N_in_service: 2.9, utilization: 292.3%)
  Using achieved RPS from summary: 200.0 -> 128.89 (64.4% efficiency)
  200.0 RPS: 1/1 pods busy (avg latency: 9610287μs, N_in_service: 1238.7, utilization: 123867.0%)
  Using achieved RPS from summary: 300.0 -> 111.53 (37.2% efficiency)
  300.0 RPS: 1/1 pods busy (avg latency: 19539372μs, N_in_service: 2179.2, utilization: 217922.6%)
  Using achieved RPS from summary: 400.0 -> 99.34 (24.8% efficiency)
  400.0 RPS: 1/1 pods busy (avg latency: 19840444μs, N_in_service: 1970.9, utilization: 197095.0%)
  Using achieved RPS from summary: 500.0 -> 95.43 (19.1% efficiency)
  500.0 RPS: 1/1 pods busy (avg latency: 15005411μs, N_in_service: 1432.0, utilization: 143196.6%)
Processed: 5 configurations

--- Processing: fibonacci_nodejs_a1_statistics.csv ---
Found achieved RPS data: results_fibonacci_nodejs_a1/achieved_rps_summary.csv
Loaded achieved RPS for targets: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]
Detected: Runtime=nodejs, Config=A1, Series=A
Configuration: 1.0vCPU × 1.0GB × 1 pods (concurrency: 1/pod)
Found data for RPS levels: [100, 200, 300, 400, 500]
  Using achieved RPS from summary: 100.0 -> 98.53 (98.5% efficiency)
  100.0 RPS: 1/1 pods busy (avg latency: 27170μs, N_in_service: 2.7, utilization: 267.7%)
  Using achieved RPS from summary: 200.0 -> 126.99 (63.5% efficiency)
  200.0 RPS: 1/1 pods busy (avg latency: 9914464μs, N_in_service: 1259.0, utilization: 125903.8%)
  Using achieved RPS from summary: 300.0 -> 113.16 (37.7% efficiency)
  300.0 RPS: 1/1 pods busy (avg latency: 18479434μs, N_in_service: 2091.1, utilization: 209113.3%)
  Using achieved RPS from summary: 400.0 -> 98.14 (24.5% efficiency)
  400.0 RPS: 1/1 pods busy (avg latency: 17598338μs, N_in_service: 1727.1, utilization: 172710.1%)
  Using achieved RPS from summary: 500.0 -> 81.75 (16.4% efficiency)
  500.0 RPS: 1/1 pods busy (avg latency: 9256752μs, N_in_service: 756.7, utilization: 75673.9%)
Processed: 5 configurations

--- Processing: fibonacci_nodejs_a2_statistics.csv ---
Found achieved RPS data: results_fibonacci_nodejs_a2/achieved_rps_summary.csv
Loaded achieved RPS for targets: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]
Detected: Runtime=nodejs, Config=A2, Series=A
Configuration: 0.5vCPU × 0.5GB × 1 pods (concurrency: 1/pod)
Found data for RPS levels: [100, 200, 300, 400, 500]
  Using achieved RPS from summary: 100.0 -> 99.19 (99.2% efficiency)
  100.0 RPS: 1/1 pods busy (avg latency: 29692μs, N_in_service: 2.9, utilization: 294.5%)
  Using achieved RPS from summary: 200.0 -> 121.62 (60.8% efficiency)
  200.0 RPS: 1/1 pods busy (avg latency: 10573091μs, N_in_service: 1285.9, utilization: 128589.9%)
  Using achieved RPS from summary: 300.0 -> 103.22 (34.4% efficiency)
  300.0 RPS: 1/1 pods busy (avg latency: 19990136μs, N_in_service: 2063.4, utilization: 206338.2%)
  Using achieved RPS from summary: 400.0 -> 92.90 (23.2% efficiency)
  400.0 RPS: 1/1 pods busy (avg latency: 20485057μs, N_in_service: 1903.1, utilization: 190306.2%)
  Using achieved RPS from summary: 500.0 -> 101.15 (20.2% efficiency)
  500.0 RPS: 1/1 pods busy (avg latency: 10547579μs, N_in_service: 1066.9, utilization: 106688.8%)
Processed: 5 configurations

--- Processing: fibonacci_nodejs_a3_statistics.csv ---
Found achieved RPS data: results_fibonacci_nodejs_a3/achieved_rps_summary.csv
Loaded achieved RPS for targets: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]
Detected: Runtime=nodejs, Config=A3, Series=A
Configuration: 0.25vCPU × 0.25GB × 1 pods (concurrency: 1/pod)
Found data for RPS levels: [100, 200, 300, 400, 500]
  Using achieved RPS from summary: 100.0 -> 99.17 (99.2% efficiency)
  100.0 RPS: 1/1 pods busy (avg latency: 31252μs, N_in_service: 3.1, utilization: 309.9%)
  Using achieved RPS from summary: 200.0 -> 126.85 (63.4% efficiency)
  200.0 RPS: 1/1 pods busy (avg latency: 10065132μs, N_in_service: 1276.8, utilization: 127676.2%)
  Using achieved RPS from summary: 300.0 -> 100.85 (33.6% efficiency)
  300.0 RPS: 1/1 pods busy (avg latency: 20195953μs, N_in_service: 2036.8, utilization: 203676.2%)
  Using achieved RPS from summary: 400.0 -> 89.36 (22.3% efficiency)
  400.0 RPS: 1/1 pods busy (avg latency: 19320210μs, N_in_service: 1726.5, utilization: 172645.4%)
  Using achieved RPS from summary: 500.0 -> 80.53 (16.1% efficiency)
  500.0 RPS: 1/1 pods busy (avg latency: 6757168μs, N_in_service: 544.2, utilization: 54415.5%)
Processed: 5 configurations

--- Processing: fibonacci_nodejs_a4_statistics.csv ---
Found achieved RPS data: results_fibonacci_nodejs_a4/achieved_rps_summary.csv
Loaded achieved RPS for targets: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]
Detected: Runtime=nodejs, Config=A4, Series=A
Configuration: 0.125vCPU × 0.125GB × 1 pods (concurrency: 1/pod)
Found data for RPS levels: [100, 200, 300, 400, 500]
  Using achieved RPS from summary: 100.0 -> 99.00 (99.0% efficiency)
  100.0 RPS: 1/1 pods busy (avg latency: 41980μs, N_in_service: 4.2, utilization: 415.6%)
  Using achieved RPS from summary: 200.0 -> 121.62 (60.8% efficiency)
  200.0 RPS: 1/1 pods busy (avg latency: 10722294μs, N_in_service: 1304.0, utilization: 130404.5%)
  Using achieved RPS from summary: 300.0 -> 97.82 (32.6% efficiency)
  300.0 RPS: 1/1 pods busy (avg latency: 20421970μs, N_in_service: 1997.7, utilization: 199767.7%)
  Using achieved RPS from summary: 400.0 -> 95.09 (23.8% efficiency)
  400.0 RPS: 1/1 pods busy (avg latency: 19500889μs, N_in_service: 1854.3, utilization: 185434.0%)
  Using achieved RPS from summary: 500.0 -> 90.07 (18.0% efficiency)
  500.0 RPS: 1/1 pods busy (avg latency: 20956970μs, N_in_service: 1887.6, utilization: 188759.4%)
Processed: 5 configurations

--- Processing: fibonacci_nodejs_a5_statistics.csv ---
Found achieved RPS data: results_fibonacci_nodejs_a5/achieved_rps_summary.csv
Loaded achieved RPS for targets: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]
Detected: Runtime=nodejs, Config=A5, Series=A
Configuration: 0.1vCPU × 0.1GB × 1 pods (concurrency: 1/pod)
Found data for RPS levels: [100, 200, 300, 400, 500]
  Using achieved RPS from summary: 100.0 -> 99.14 (99.1% efficiency)
  100.0 RPS: 1/1 pods busy (avg latency: 54420μs, N_in_service: 5.4, utilization: 539.5%)
  Using achieved RPS from summary: 200.0 -> 119.79 (59.9% efficiency)
  200.0 RPS: 1/1 pods busy (avg latency: 10952698μs, N_in_service: 1312.0, utilization: 131202.4%)
  Using achieved RPS from summary: 300.0 -> 94.64 (31.5% efficiency)
  300.0 RPS: 1/1 pods busy (avg latency: 20855088μs, N_in_service: 1973.7, utilization: 197372.6%)
  Using achieved RPS from summary: 400.0 -> 82.06 (20.5% efficiency)
  400.0 RPS: 1/1 pods busy (avg latency: 20705037μs, N_in_service: 1699.1, utilization: 169905.5%)
  Using achieved RPS from summary: 500.0 -> 88.91 (17.8% efficiency)
  500.0 RPS: 1/1 pods busy (avg latency: 15930720μs, N_in_service: 1416.4, utilization: 141640.0%)
Processed: 5 configurations

--- Processing: fibonacci_python_a1_statistics.csv ---
Found achieved RPS data: results_fibonacci_python_a1/achieved_rps_summary.csv
Loaded achieved RPS for targets: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]
Detected: Runtime=python, Config=A1, Series=A
Configuration: 1.0vCPU × 1.0GB × 1 pods (concurrency: 1/pod)
Found data for RPS levels: [100, 200, 300, 400, 500]
  Using achieved RPS from summary: 100.0 -> 98.82 (98.8% efficiency)
  100.0 RPS: 1/1 pods busy (avg latency: 34153μs, N_in_service: 3.4, utilization: 337.5%)
  Using achieved RPS from summary: 200.0 -> 122.20 (61.1% efficiency)
  200.0 RPS: 1/1 pods busy (avg latency: 10770798μs, N_in_service: 1316.2, utilization: 131619.2%)
  Using achieved RPS from summary: 300.0 -> 107.44 (35.8% efficiency)
  300.0 RPS: 1/1 pods busy (avg latency: 19565854μs, N_in_service: 2102.2, utilization: 210215.5%)
  Using achieved RPS from summary: 400.0 -> 99.47 (24.9% efficiency)
  400.0 RPS: 1/1 pods busy (avg latency: 20667261μs, N_in_service: 2055.8, utilization: 205577.2%)
  Using achieved RPS from summary: 500.0 -> 94.86 (19.0% efficiency)
  500.0 RPS: 1/1 pods busy (avg latency: 4521201μs, N_in_service: 428.9, utilization: 42888.1%)
Processed: 5 configurations

--- Processing: fibonacci_python_a2_statistics.csv ---
Found achieved RPS data: results_fibonacci_python_a2/achieved_rps_summary.csv
Loaded achieved RPS for targets: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]
Detected: Runtime=python, Config=A2, Series=A
Configuration: 0.5vCPU × 0.5GB × 1 pods (concurrency: 1/pod)
Found data for RPS levels: [100, 200, 300, 400, 500]
  Using achieved RPS from summary: 100.0 -> 98.99 (99.0% efficiency)
  100.0 RPS: 1/1 pods busy (avg latency: 31697μs, N_in_service: 3.1, utilization: 313.8%)
  Using achieved RPS from summary: 200.0 -> 122.71 (61.4% efficiency)
  200.0 RPS: 1/1 pods busy (avg latency: 10767554μs, N_in_service: 1321.3, utilization: 132128.7%)
  Using achieved RPS from summary: 300.0 -> 107.11 (35.7% efficiency)
  300.0 RPS: 1/1 pods busy (avg latency: 19367212μs, N_in_service: 2074.4, utilization: 207442.2%)
  Using achieved RPS from summary: 400.0 -> 95.17 (23.8% efficiency)
  400.0 RPS: 1/1 pods busy (avg latency: 20142878μs, N_in_service: 1917.0, utilization: 191699.8%)
  Using achieved RPS from summary: 500.0 -> 112.02 (22.4% efficiency)
  500.0 RPS: 1/1 pods busy (avg latency: 12062734μs, N_in_service: 1351.3, utilization: 135126.7%)
Processed: 5 configurations

--- Processing: fibonacci_python_a3_statistics.csv ---
Found achieved RPS data: results_fibonacci_python_a3/achieved_rps_summary.csv
Loaded achieved RPS for targets: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]
Detected: Runtime=python, Config=A3, Series=A
Configuration: 0.25vCPU × 0.25GB × 1 pods (concurrency: 1/pod)
Found data for RPS levels: [100, 200, 300, 400, 500]
  Using achieved RPS from summary: 100.0 -> 99.13 (99.1% efficiency)
  100.0 RPS: 1/1 pods busy (avg latency: 35370μs, N_in_service: 3.5, utilization: 350.6%)
  Using achieved RPS from summary: 200.0 -> 116.54 (58.3% efficiency)
  200.0 RPS: 1/1 pods busy (avg latency: 11423462μs, N_in_service: 1331.3, utilization: 133129.0%)
  Using achieved RPS from summary: 300.0 -> 105.37 (35.1% efficiency)
  300.0 RPS: 1/1 pods busy (avg latency: 19895107μs, N_in_service: 2096.3, utilization: 209634.7%)
  Using achieved RPS from summary: 400.0 -> 82.94 (20.7% efficiency)
  400.0 RPS: 1/1 pods busy (avg latency: 21048796μs, N_in_service: 1745.8, utilization: 174578.7%)
  Using achieved RPS from summary: 500.0 -> 104.36 (20.9% efficiency)
  500.0 RPS: 1/1 pods busy (avg latency: 5611642μs, N_in_service: 585.6, utilization: 58563.1%)
Processed: 5 configurations

--- Processing: fibonacci_python_a4_statistics.csv ---
Found achieved RPS data: results_fibonacci_python_a4/achieved_rps_summary.csv
Loaded achieved RPS for targets: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]
Detected: Runtime=python, Config=A4, Series=A
Configuration: 0.125vCPU × 0.125GB × 1 pods (concurrency: 1/pod)
Found data for RPS levels: [100, 200, 300, 400, 500]
  Using achieved RPS from summary: 100.0 -> 99.13 (99.1% efficiency)
  100.0 RPS: 1/1 pods busy (avg latency: 48476μs, N_in_service: 4.8, utilization: 480.5%)
  Using achieved RPS from summary: 200.0 -> 107.92 (54.0% efficiency)
  200.0 RPS: 1/1 pods busy (avg latency: 13062075μs, N_in_service: 1409.7, utilization: 140965.9%)
  Using achieved RPS from summary: 300.0 -> 84.87 (28.3% efficiency)
  300.0 RPS: 1/1 pods busy (avg latency: 21664044μs, N_in_service: 1838.6, utilization: 183862.7%)
  Using achieved RPS from summary: 400.0 -> 75.01 (18.8% efficiency)
  400.0 RPS: 1/1 pods busy (avg latency: 21191343μs, N_in_service: 1589.6, utilization: 158956.3%)
  Using achieved RPS from summary: 500.0 -> 74.91 (15.0% efficiency)
  500.0 RPS: 1/1 pods busy (avg latency: 18673484μs, N_in_service: 1398.8, utilization: 139883.1%)
Processed: 5 configurations

--- Processing: fibonacci_python_a5_statistics.csv ---
Found achieved RPS data: results_fibonacci_python_a5/achieved_rps_summary.csv
Loaded achieved RPS for targets: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]
Detected: Runtime=python, Config=A5, Series=A
Configuration: 0.1vCPU × 0.1GB × 1 pods (concurrency: 1/pod)
Found data for RPS levels: [100, 200, 300, 400, 500]
  Using achieved RPS from summary: 100.0 -> 94.79 (94.8% efficiency)
  100.0 RPS: 1/1 pods busy (avg latency: 1100577μs, N_in_service: 104.3, utilization: 10432.4%)
  Using achieved RPS from summary: 200.0 -> 81.75 (40.9% efficiency)
  200.0 RPS: 1/1 pods busy (avg latency: 17329962μs, N_in_service: 1416.7, utilization: 141672.4%)
  Using achieved RPS from summary: 300.0 -> 73.50 (24.5% efficiency)
  300.0 RPS: 1/1 pods busy (avg latency: 22742909μs, N_in_service: 1671.6, utilization: 167160.4%)
  Using achieved RPS from summary: 400.0 -> 61.11 (15.3% efficiency)
  400.0 RPS: 1/1 pods busy (avg latency: 23256625μs, N_in_service: 1421.2, utilization: 142121.2%)
  Using achieved RPS from summary: 500.0 -> 75.32 (15.1% efficiency)
  500.0 RPS: 1/1 pods busy (avg latency: 11199851μs, N_in_service: 843.6, utilization: 84357.3%)
Processed: 5 configurations

Total configurations processed: 75

================================================================================
MULTI-POD CLOUD RUN COST ANALYSIS
================================================================================

SERIES SCALING STRATEGIES:
  A-Series: Single pod scale-up (varying resources per pod)
  B-Series: Multi-pod scale-out (constant total resources)
  C-Series: Multi-pod scale-out (higher total resources)

A-SERIES ANALYSIS:
  Strategy: Single pod, varying resources
  A1: 1.0vCPU × 1.0GB × 1 pod
  A2: 0.5vCPU × 0.5GB × 1 pod
  A3: 0.25vCPU × 0.25GB × 1 pod
  A4: 0.125vCPU × 0.125GB × 1 pod
  A5: 0.1vCPU × 0.1GB × 1 pod
  Average instance cost: $0.000077
  Average request cost: $0.125750
  Instance cheaper: 75/75 (100.0%)

CROSS-SERIES COST COMPARISON:
       instance_cost_per_1k                     request_cost_per_1k                     num_pods         total_vcpu          
                       mean       min       max                mean       min       max     mean min max       mean  min  max
series                                                                                                                       
A                  0.000077  0.000016  0.000245             0.12575  0.000478  0.551509      1.0   1   1      0.395  0.1  1.0

BUSY PODS & FLEET UTILIZATION BY RPS LEVEL:
  100 RPS:
    A-Series: 1.0-1.0 busy pods (avg: 1.0), utilization: 253.8%-10432.4% (avg: 1006.8%)
  200 RPS:
    A-Series: 1.0-1.0 busy pods (avg: 1.0), utilization: 117326.9%-141672.4% (avg: 128302.3%)
  300 RPS:
    A-Series: 1.0-1.0 busy pods (avg: 1.0), utilization: 167160.4%-219721.0% (avg: 204986.4%)
  400 RPS:
    A-Series: 1.0-1.0 busy pods (avg: 1.0), utilization: 142121.2%-208581.5% (avg: 184793.8%)
  500 RPS:
    A-Series: 1.0-1.0 busy pods (avg: 1.0), utilization: 42888.1%-205981.6% (avg: 115839.8%)

MULTI-POD PENALTY ANALYSIS:

BREAK-EVEN ANALYSIS (Request = Instance cost):
Break-even latency per configuration (when request-based becomes competitive):
  A1 (1 pods, 1.0vCPU): -7.5ms
  A2 (1 pods, 0.5vCPU): -22.6ms
  A3 (1 pods, 0.25vCPU): -52.8ms
  A4 (1 pods, 0.125vCPU): -113.2ms
  A5 (1 pods, 0.1vCPU): -143.4ms

RUNTIME PERFORMANCE:
  Go: Instance avg=$0.000073, Request avg=$0.128273
  Nodejs: Instance avg=$0.000078, Request avg=$0.123346
  Python: Instance avg=$0.000078, Request avg=$0.125630
Detailed results saved to: a_series_analysis/multi_pod_cost_analysis.csv
Series summary saved to: a_series_analysis/series_summary.csv
Multi-pod penalty analysis saved to: a_series_analysis/multi_pod_penalty_analysis.csv
Series comparison chart saved to: a_series_analysis/multi_pod_series_comparison.png

✓ Multi-pod analysis completed!
✓ Results saved in: a_series_analysis/
